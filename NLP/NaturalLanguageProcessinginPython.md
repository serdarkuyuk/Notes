NLP Projects

https://www.youtube.com/watch?v=xvqsFTUsOmc

# Project 1 - Comadian

Tokinization
Split text into smallar pieces aka a token. The most common token size is a word. It can also be a sentence

stop words
the a etc...

1. **Corpus** - a collection of text
2. **Document-Term Matrix** - word counts in matrix format

bag of words model
this representation of the text is called a bag of words model. It is a simple format that ignores order

stemming/lemmatization
drive, driving, drived

parts of speech tagging
